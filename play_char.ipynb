{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Character Level GPT on Text Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%d/%m/%Y %H:%M:%S\",\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print(\"data has %d characters, %d unique.\" % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i:ch for i, ch in enumerate(chars)}\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx : idx+self.block_size+1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "\n",
    "        x = torch.tensor(dix[:-1], dtype = torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype = torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 509351 characters, 86 unique.\n"
     ]
    }
   ],
   "source": [
    "text = open(\"./fairytales.txt\", \"r\").read()\n",
    "train_dataset = CharDataset(text, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/05/2021 13:21:17 - INFO - model - Number of parameters : 2.532454e+07\n"
     ]
    }
   ],
   "source": [
    "from model import GPT, GPTconfig\n",
    "mconf = GPTconfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer, TrainerConfig\n",
    "\n",
    "tconf = TrainerConfig(max_epochs=5, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 994: train loss 1.11714. lr 0.0003001480838414411: 100%|██████████| 995/995 [26:07<00:00,  1.58s/it] \n",
      "epoch 2 iter 994: train loss 0.90305. lr 5.9999999999999995e-05: 100%|██████████| 995/995 [26:05<00:00,  1.57s/it]\n",
      "epoch 3 iter 994: train loss 0.84035. lr 0.00030014808384144087: 100%|██████████| 995/995 [26:06<00:00,  1.57s/it]\n",
      "epoch 4 iter 994: train loss 0.71084. lr 0.0005999998538078394: 100%|██████████| 995/995 [26:46<00:00,  1.61s/it] \n",
      "epoch 5 iter 994: train loss 0.49762. lr 0.00029955574862000156: 100%|██████████| 995/995 [26:56<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./saved_models/fairytale-05-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/05/2021 21:33:47 - INFO - model - Number of parameters : 2.532966e+07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPT, GPTconfig\n",
    "mconf = GPTconfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)\n",
    "model.load_state_dict(torch.load(\"./saved_models/fairytale-05-epochs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text using Sample as starter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun shone in the sky.\"\n",
      "\"It is not so, nor it was not so,\" said Mr. Fox.\n",
      "\"And then—and then I opened the door, and when no can yet do a string, and dragged it along after you.\" \"I'll do so another time,\" replied Jack.\n",
      "On Wednesday, Jack went back and told them to fly before him.\"\n",
      "\"Alas!\" quoth the prince, \"what shall we do to him?\" said one to the other.\n",
      "\"I have many ranks,\" said Tom, \"my mother did not often care to cross him; indeed, the more she struggled and fought as if she could find a place. So off he went there he should hear joyful news. He made little count of them all. Then he took out it and got into the garden for the apples, he could see all that was passing in the world. And he went there was a little boy, and two well-diggers, and two ditch-diggers, and a bear, and a wolf, and I can outrun you too-o-o-o!\"\n",
      "\"Ye can. can ye?\" growled the bear, \"we'll see about that!\" and trotted as fast as he comes. He said: \"What have I done? I promised to give the giant who carried me over the river on his back. Now when the old nurse brought him the baby, he swore that he had move still play a farewell of the King of Elfland in, Your fortune is forlorn.\"\n",
      "Then the ogre sat down to the ground.\n",
      "He ran, and he ran, until he could run no longer, and then he took down the sack, and began to batter it, and then he turned back home again, and on one leg and saying:\n",
      "\"I've taken you for clothe and have jumped out of her skin for joy, but she didn't so much as know how to get any,\" answered Dick. \"If you are willing, come along with me,\" said she, \"no thieves will ever find it now, that's your pay?\" says she.\n",
      "That looked out o' the window in its place, for she had no business there.\n",
      "Well, my young son, how are you, my old boy? You seem old so enough to work; I am afraid you are inclined to be lazy.\"\n",
      "\"No, indeed, sir, had you not once a young daughter whom you would not understand all, and turned to the King.\"Father!\" he said, for it was the Prince himself, \"I have made my choice, and here is my brid\n"
     ]
    }
   ],
   "source": [
    "from utils import sample\n",
    "\n",
    "context = \"The sun shone in the sky.\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
